# Inferring phylogenies from data

In the previous chapter we built modeling machinery that allowed us to specify a rate matrix $\mathbf{Q}$ (Equation \@ref(eq:sim-gtr)), and from that derive the matrix $\mathbf{P}(t)$  (Equation \@ref(eq:jc69-prob)) that gives the probability of a particular end state given the starting state and the edge length $t$. We used $\mathbf{P}(t)$ in a generative context, to simulate the evolution of a DNA sequence along the edges of a phylogeny.

We will now turn to using the same statistical framework for what may initially seem to be a very different task -- phylogenetic inference, where we infer the topology and edge lengths of a phylogeny from character data (DNA sequences, in this case). But these tasks are similar conceptually. The basic intuition is that you can infer a phylogeny by looking for the topology and edge lengths that are most probable to generate the observed data [@Felsenstein:1981vk].

This relationship between simulation and inference is widely used in a variety of fields. The probability of the observed data given a hypothesis is referred to as the Likelihood. Searching for the most likely hypothesis is referred to as Maximum Likelihood (ML).

## Probability of a single history

We will consider the toy phylogeny, along with its tip states, shown in Figure \@ref(fig:inference-toy).

```{r inference-toy, fig.cap="The toy phylogeny we will use to examine inference. Node numbers are in red. Edge lengths are in orange. Tip node states are within boxes."}

phy_text = "(((Species_A:0.5,Species_B:0.5):0.5,Species_C:1.0):0.2,Species_D:1.2);"
phy = read.tree( text=phy_text )

internal_states = expand.grid(
  node_5=c("A", "C", "G", "T"), 
  node_6=c("A", "C", "G", "T"), 
  node_7=c("A", "C", "G", "T")) %>% 
as.matrix() %>% 
t()

tip_states = c("T", "T", "A", "C")

node_states = rbind( replicate(ncol(internal_states), tip_states), internal_states )
rownames(node_states) = NULL

get_edge_states = function( phy, states){
  D = tibble(
    parent = phy$edge[,1],
    child  = phy$edge[,2],
    start  = states[ phy$edge[,1] ],
    end    = states[ phy$edge[,2] ],
    edge_length = phy$edge.length
  )
  D
}

get_edge_prob  = function( start, end, edge_length, Q ){
  P = exponentiate_matrix(Q*edge_length)
  P[ start, end ]
}

get_edge_probs = function( phy, states, Q ){
  edges = get_edge_states( phy, states)
  prob = 
    edges %>%
    select( start, end, edge_length ) %>%
    pmap( get_edge_prob, Q )
  prob %<>% unlist()
  names(prob) = NULL
  edges$prob = prob
  edges
}

get_tree_prob = function( states, phy, R, Pi ){
  Q = R %*% Pi
  colnames(Q) = c("A", "C", "G", "T")
  diag(Q) = - rowSums(Q)
  
  edge_probs = 
  get_edge_probs( phy, states, Q ) %>%
  arrange(child)
  
  p = rep( NA, max(edge_probs$child))
  p[ edge_probs$child ] = edge_probs$prob
  
  pi_v = diag(Pi)
  names(pi_v) = rownames(Pi)
  
  root_node = which(is.na(p))
  root_prob = pi_v[ states[root_node] ]
  p[root_node] = root_prob
  
  prod(p)
  
}

example_states = node_states[,60]

edge_probs = 
  get_edge_probs( phy, example_states, Q ) %>%
  arrange(child)

edge_lengths = rep( NA, max(edge_probs$child))
edge_lengths[ edge_probs$child ] = edge_probs$edge_length

p = rep( NA, max(edge_probs$child))
p[ edge_probs$child ] = edge_probs$prob

tip_state_labels = rep( NA, max(edge_probs$child))
tip_state_labels[1:length(tip_states)] = tip_states

ggtree(phy) +
  geom_tiplab(offset=0.2) +
  geom_label(aes(label=tip_state_labels)) +
  geom_text2(aes(label=node), col="red", nudge_x=0.12 ) +
  geom_text2(aes(label=edge_lengths), col="orange", nudge_x=-0.12, nudge_y=-0.1 ) +
  xlim(0,2)


```

We will start be calculating the probability of a single history of evolution for a single site on a single phylogeny with specified topology and edge lengths. This history is the full set of states at all nodes. These are added to the toy phylogeny in Figure \@ref(fig:inference-internal-states). I want to emphasize that this isn't a history we have any particular reason to believe, it is just one possible history of states randomly chosen from all the possible histories.

```{r inference-internal-states, fig.cap="The same toy tree as above, but with arbitrary internal node states (in boxes)."}

ggtree(phy) +
  geom_tiplab(offset=0.2) +
  geom_label(aes(label=example_states)) +
  geom_text2(aes(label=node), col="red", nudge_x=0.15 ) +
  geom_text2(aes(label=edge_lengths), col="orange", nudge_x=-0.12, nudge_y=-0.1 ) +
  xlim(0,2)

```

Our goal now is to calculate the probability of each observed change. Recall that the matrix that contains these probabilities, given a starting state (rows), ending state (columns), and edge length $t$, is given by:

\begin{equation} 
  \mathbf{P}\left(t\right) = e^{\mathbf{Q} t} 
  (\#eq:prob)
\end{equation}

Where $\mathbf{Q}$ is the rate matrix. Let's plug some numbers in using the model we specified in the previous chapter. We don't have any specific reason to use this model on this tree, we are just sticking with it since we already built it.

Here is the relative rate matrix $\mathbf{R}$:

```{r}
R
```

The equilibrium frequencies $\mathbf{\Pi}$:

```{r}
Pi
```

And their product $\mathbf{Q}$, with the diagonal adjusted so that rows sum to 0:

```{r}
Q
```

For each edge, we can now use $\mathbf{P}(t)$ to calculate the probability of a change from the start state at the parent node to the end state at the child node, given the edge length $t$. The results are shown in Figure \@ref(fig:inference-history). 


```{r inference-history, fig.cap="The same toy tree as above, but with probabilities of the specific change along each edge (in blue)."}


ggtree(phy) +
  geom_tiplab(offset=0.2) +
  geom_label(aes(label=example_states)) +
  geom_text2(aes(label=node), col="red", nudge_x=0.15 ) +
  geom_text2(aes(label=edge_lengths), col="orange", nudge_x=-0.12, nudge_y=-0.1 ) +
  geom_text2(aes(label=round(p,3)), col="blue", nudge_x=-0.15, nudge_y=0.1 ) +
  xlim(0,2)

```

Now that we have the probabilities of each of these changes, we can calculate the joint probability of all these changes. When we want to calculate the joint probability of multiple independent events, we take the product of the probability of each specific event. For example, the probability of rolling a 4 on a fair die is $1/6$. The probability of rolling two 4s on two fair dice is $1/6\times1/6=1/36$. So we can take the product of all the blue probabilities to calculate the joint probability of all of these events happening. 
We can think of these as the probabilities of specific changes along each edge as the probabilities of the state at each child node. 

```{r}
knitr::kable(data.frame( node=1:length(p), probability=p ))

pi_v = diag(Pi)
names(pi_v) = rownames(Pi)

root_node = which(is.na(p))
root_prob = pi_v[ example_states[root_node] ]

```

Note, though, that the probability for node 5 is missing (it has a value of `NA`, which means it is Not Available). By reference to Figure \@ref(fig:inference-history) we can see that this is the root node. This makes sense since the root is not the child of any edge, and we calculated the probabilities based on changes along edges. We will therefore assess the probability of the root node state according to $\mathbf{\Pi}$, the equilibrium frequencies. This is the same approach we took when simulating data on a tree. When we fill that in our full set of probabilities is:

```{r}

p[which(is.na(p))] = root_prob

knitr::kable(data.frame( node=1:length(p), probability=p )) %>% 
  kable_styling()

example_prob = prod(p)

```

The joint probability of all these states can now be calculated as the product of each state. This comes out to $`r example_prob`$. There are multiple ways to think about this probability. One is from a frequentist perspective. If we were to simulate character states on this tree, we would expect this full set of character states to occur at a frequency of `r round(example_prob*1e6,1)` times out of a million simulations.

Here we have used much of the same machinery as we did in the previous chapter, but toward a slightly different end. Rather than use the probability distributions to generate nucleotides in a simulation, we instead calculated the probability of a particular set of nucleotides. These may have seemed like very different tasks at first blush, but as you can now see their mathematical implementation shares many features.

## Probability of multiple histories

Above we considered the joint probability of a specific set of nucleotide states at all nodes, including both tip nodes and internal nodes. Usually, though, we don't know the internal node states. We don't even know what internal nodes exist, which is why we are trying to infer the phylogeny! Instead we have observed states that we got by sequencing organisms at the tips. We want to clamp these tip states and assess their probability on a particular tree (with edge lengths) under the model. This probability is independent of a specific history of internal node states. 

If we aren't clamping the internal node states as well, how can we calculate the probability of just the tip node states? The key is to consider all possible internal states. Each configuration of internal node states represents one possible history that gave rise to the observed tip states. We can sum the probabilities of each of these different ways to get the tip states to find the probability of the tip states over all possible histories. We are summing the probabilities because these are mutually exclusive histories that could give rise to the observed data. For example, if we want to find the probability of getting a total of seven when rolling two dice, we need to add up the probability of each way to get seven (1+6 *or* 2+5 *or* 3+4 *or* ... 6+1). This is different from when we multiplied probabilities to find the joint probabilities of multiple events occurring together (*e.g.*, the probability of rolling a 4 *and* another 4).

This is a small tree, with only 3 internal nodes that can each have 4 states. This gives $4^3=64$ possible histories. That is small enough to list them out below. I also include the probability of each specific history, calculated exactly as I did above (the example above corresponds to row 60 here).

```{r}

# Get a list of node state vectors

node_states_list = lapply(seq_len(ncol(node_states)), function(i) node_states[,i])



history_probs = 
  node_states_list %>% 
  map( get_tree_prob, phy, R, Pi ) %>%
  unlist()

history_D = as.data.frame(t(node_states))
names(history_D) = c("n1", "n2", "n3", "n4", "n5", "n6", "n7")
history_D$probability = history_probs

knitr::kable( history_D ) %>% 
  kable_styling()


```

Note that I listed the states for all the nodes, including nodes 1-4, which are clamped. It is the last three internal nodes (n5-n7) that have variable states. The probabilities for each specific history range quite widely, from $`r min(history_probs)`$ to $`r max(history_probs)`$.

The sum of the probabilities for each of these different histories for n5-n7 that give rise to the observed clamped states for tip nodes n1-n4 is $`r sum(history_probs)`$. This probability of the data given a particular hypothesis (the topology, edge lengths, model, and model parameters) is the likelihood.

## Log likelihood

The likelihood of these data on this phylogeny, $`r sum(history_probs)`$, is not a big number. And this is a very small tree. As trees get larger there are many more probabilities we need to multiply, so the products get even smaller. The joint probabilities, in fact, get so small that computers have trouble storing them efficiently. Rather than store and manipulate the small probabilities directly, most tools take the natural logs of the probabilities, $ln(p)$. The log likelihood for this phylogeny is $`r sum(history_probs) %>% log()`$. Taking the log transforms probabilities to a numerical representation that is easier to work with. It also has the added value of making calculations of joint probability simpler. Given the relationship between the log of products of variables and the sum of logs of each value:

\begin{equation} 
  ln(a)+ln(b) = ln(ab)
  (\#eq:logs)
\end{equation}

We can calculate joint log probabilities as sums of log probabilities for each event, rather than as the log of products of the probabilities. Addition is much faster than multiplication for computers (since multiplication is a series of addition operations), so this speeds things up. For these reasons you will almost always see log likelihoods, rather than just likelihoods, published in the literature. Note that because likelihoods are probabilities and therefore range from 0--1, the log likelihoods will range from $-\infty$ (for probabilities very close to 0) to $0$ (for probabilities close to 1). Since likelihoods tend to be small, they end up as log likelihoods that are negative numbers with large absolute values.

## Likelihood for multiple sites

The machinery above gives us everything we need to calculate the log likelihood of a specific pattern of nucleotides across tips for a single site in a DNA sequence. We new need to expand this model from a single site to multiple sites within a gene or even across whole genomes.

This comes down to more of the same. We do everything we did above for each site, and then sum the log likelihoods across sites. This gives us the joint probability of observing the data seen across tips for each site in the DNA sequence. This joint probability for all sites will be much smaller than the probability for each individual site.


## Maximum likelihood

At this point we can calculate the log likelihood for specified phylogenies, models, and DNA sequences. But we set out to do phylogenetic inference, where we estimate phylogenies from sequences at tips. How do we get there from here? Once we can calculate the likelihood of a given phylogeny, we can calculate the likelihood of any phylogeny. We can then search for the phylogeny with the maximum likelihood (and, of course, maximum log likelihood).

The small toy phylogeny considered here (Figure \@ref(fig:inference-toy)) has four tip nodes. Be reference to Equation \@ref(eq:ntrees), we can see that there are 15 possible topologies. For each, we can optimize the edge lengths to find the maximum likelihood for the topology. This is an iterative process, where each edge length is progressively refined until no change increases the likelihood. This excellent [interactive visualization](http://phylo.bio.ku.edu/mephytis/brlen-opt.html) allows you to manually optimize edge lengths on a small phylogeny. Then we pick the topology with the maximum likelihood. This requires a very large number of calculations, but is doable for every possible topology.

Things change very quickly, though, as trees grow in size. Beyond about 15 tips there are so many possible topologies that it is impossible to calculate the likelihood for every topology using existing computer hardware and software. That means it is necessary to use heuristics - to modify the tree you have until you can do no better. This is like hill climbing. You calculate the likelihood of a tree and then modify it. If the likelihood is higher, you keep it, if it is worse, you discard it.

This might sound simple, but it isn't. One challenge is that you can get trapped on a local maximum and mistake it for the best phylogeny, when in fact there are other phylogenies with very different topologies that are much better but not locally accessible. There is extraordinary craft that goes into building tools that are able to efficiently climb these likelihood surfaces without getting trapped in local maxima.

Optimization of calculations becomes very important. For example, it isn't necessary to recalculate all values on each new topology, since some of the calculations from previous topologies remain relevant [@Felsenstein:1981vk].

## Optimality criteria

Here we used likelihood as an optimality criterion to search over treespace, the set of all possible phylogenies, to find the phylogeny that maximizes the criterion. There are other optimality criteria that are used in phylogenetic inference. These include parsimony. In parsimony, the minimum number of changes along edges needed to explain the data at the tips is used as the criterion to evaluate each topology. Optimization proceeds by attempting to identify the topology that requires the fewest changes. This requires far less computational power than likelihood, so searches are faster.

Under some conditions parsimony and likelihood will recover similar topologies, but often they do not. This is because they are doing different things that under many conditions lead to different results [@steel2000parsimony]. For example, we do not always expect the simplest possible explanation for a given pattern to be the best explanation. If a character has a high rate of evolutionary change, then we expect many changes on a tree rather than the fewest possible. This is accommodated in a likelihood framework.
